{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2-K9xQZtEZb",
        "outputId": "77eab72c-0c7e-4e9d-f6c0-668ee325d292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGFzYDisRwm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "LstqiVWmtLvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/dataset\"\n",
        "\n",
        "full_dataset = datasets.ImageFolder(\n",
        "    root=dataset_path,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print(\"Total images:\", len(full_dataset))\n",
        "print(\"Number of classes:\", len(full_dataset.classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0JtPl5CtO47",
        "outputId": "bd73605b-0f89-491e-9e94-d6357ab136a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 9148\n",
            "Number of classes: 102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size]\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Val size:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzHGplpUtf4f",
        "outputId": "6f352c37-f5a2-47b7-df79-75eb1577dca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 7318\n",
            "Val size: 1830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=0   # <-- CHANGE THIS\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")"
      ],
      "metadata": {
        "id": "s1-b2iLBtx-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, len(full_dataset.classes))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"ResNet ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKBse83ttz-z",
        "outputId": "b28a2692-5730-4c86-e8d7-18f3364f2bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.0001\n",
        ")"
      ],
      "metadata": {
        "id": "l5lJjEL4t3XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLtcjLoeupIs",
        "outputId": "0090243a-b2d6-4621-9808-f655cf7d9aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=102, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "print_interval = 1  # ðŸ‘ˆ change this if you want\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "\n",
        "    # -------- TRAIN --------\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # ðŸ”¥ Print every 100 batches\n",
        "        if (batch_idx + 1) % print_interval == 0:\n",
        "            avg_loss = running_loss / print_interval\n",
        "            print(f\"Batch [{batch_idx+1}/{len(train_loader)}] \"\n",
        "                  f\"Avg Loss: {avg_loss:.4f}\")\n",
        "            running_loss = 0\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # -------- VALIDATION --------\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"Validation F1-score (macro): {val_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTGLOk2zt6UQ",
        "outputId": "214c0f5c-b886-49bc-dbfd-b62d96046a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Epoch 1/10 =====\n",
            "Batch [1/229] Avg Loss: 1.7560\n",
            "Batch [2/229] Avg Loss: 1.5199\n",
            "Batch [3/229] Avg Loss: 1.5702\n",
            "Batch [4/229] Avg Loss: 1.5926\n",
            "Batch [5/229] Avg Loss: 1.7427\n",
            "Batch [6/229] Avg Loss: 1.7558\n",
            "Batch [7/229] Avg Loss: 2.1463\n",
            "Batch [8/229] Avg Loss: 2.0018\n",
            "Batch [9/229] Avg Loss: 1.6914\n",
            "Batch [10/229] Avg Loss: 1.3878\n",
            "Batch [11/229] Avg Loss: 2.0633\n",
            "Batch [12/229] Avg Loss: 1.1749\n",
            "Batch [13/229] Avg Loss: 1.7714\n",
            "Batch [14/229] Avg Loss: 1.4348\n",
            "Batch [15/229] Avg Loss: 1.7931\n",
            "Batch [16/229] Avg Loss: 1.5965\n",
            "Batch [17/229] Avg Loss: 1.8184\n",
            "Batch [18/229] Avg Loss: 1.8854\n",
            "Batch [19/229] Avg Loss: 1.6834\n",
            "Batch [20/229] Avg Loss: 2.0175\n",
            "Batch [21/229] Avg Loss: 1.4014\n",
            "Batch [22/229] Avg Loss: 1.3226\n",
            "Batch [23/229] Avg Loss: 1.2379\n",
            "Batch [24/229] Avg Loss: 1.7459\n",
            "Batch [25/229] Avg Loss: 1.3290\n",
            "Batch [26/229] Avg Loss: 1.5668\n",
            "Batch [27/229] Avg Loss: 1.2972\n",
            "Batch [28/229] Avg Loss: 1.6803\n",
            "Batch [29/229] Avg Loss: 1.5630\n",
            "Batch [30/229] Avg Loss: 1.5931\n",
            "Batch [31/229] Avg Loss: 2.0121\n",
            "Batch [32/229] Avg Loss: 1.2560\n",
            "Batch [33/229] Avg Loss: 1.7007\n",
            "Batch [34/229] Avg Loss: 1.3126\n",
            "Batch [35/229] Avg Loss: 1.6709\n",
            "Batch [36/229] Avg Loss: 1.2826\n",
            "Batch [37/229] Avg Loss: 1.5644\n",
            "Batch [38/229] Avg Loss: 1.4843\n",
            "Batch [39/229] Avg Loss: 1.4814\n",
            "Batch [40/229] Avg Loss: 1.3403\n",
            "Batch [41/229] Avg Loss: 1.3559\n",
            "Batch [42/229] Avg Loss: 1.6793\n",
            "Batch [43/229] Avg Loss: 1.1864\n",
            "Batch [44/229] Avg Loss: 1.5076\n",
            "Batch [45/229] Avg Loss: 1.7604\n",
            "Batch [46/229] Avg Loss: 1.3425\n",
            "Batch [47/229] Avg Loss: 1.1112\n",
            "Batch [48/229] Avg Loss: 1.4049\n",
            "Batch [49/229] Avg Loss: 1.4987\n",
            "Batch [50/229] Avg Loss: 1.3591\n",
            "Batch [51/229] Avg Loss: 0.8211\n",
            "Batch [52/229] Avg Loss: 1.4661\n",
            "Batch [53/229] Avg Loss: 1.2394\n",
            "Batch [54/229] Avg Loss: 1.1718\n",
            "Batch [55/229] Avg Loss: 1.7758\n",
            "Batch [56/229] Avg Loss: 1.0628\n",
            "Batch [57/229] Avg Loss: 1.1763\n",
            "Batch [58/229] Avg Loss: 0.9357\n",
            "Batch [59/229] Avg Loss: 1.0537\n",
            "Batch [60/229] Avg Loss: 1.0590\n",
            "Batch [61/229] Avg Loss: 1.0488\n",
            "Batch [62/229] Avg Loss: 1.2201\n",
            "Batch [63/229] Avg Loss: 1.2597\n",
            "Batch [64/229] Avg Loss: 1.0108\n",
            "Batch [65/229] Avg Loss: 1.0096\n",
            "Batch [66/229] Avg Loss: 1.5042\n",
            "Batch [67/229] Avg Loss: 1.0024\n",
            "Batch [68/229] Avg Loss: 0.9895\n",
            "Batch [69/229] Avg Loss: 1.4872\n",
            "Batch [70/229] Avg Loss: 1.1832\n",
            "Batch [71/229] Avg Loss: 1.1821\n",
            "Batch [72/229] Avg Loss: 1.2113\n",
            "Batch [73/229] Avg Loss: 0.9783\n",
            "Batch [74/229] Avg Loss: 1.2170\n",
            "Batch [75/229] Avg Loss: 1.0684\n",
            "Batch [76/229] Avg Loss: 1.3322\n",
            "Batch [77/229] Avg Loss: 1.3204\n",
            "Batch [78/229] Avg Loss: 1.3346\n",
            "Batch [79/229] Avg Loss: 0.6747\n",
            "Batch [80/229] Avg Loss: 1.1724\n",
            "Batch [81/229] Avg Loss: 1.0476\n",
            "Batch [82/229] Avg Loss: 1.3840\n",
            "Batch [83/229] Avg Loss: 1.1325\n",
            "Batch [84/229] Avg Loss: 1.1264\n",
            "Batch [85/229] Avg Loss: 0.8173\n",
            "Batch [86/229] Avg Loss: 0.8673\n",
            "Batch [87/229] Avg Loss: 1.2490\n",
            "Batch [88/229] Avg Loss: 1.0251\n",
            "Batch [89/229] Avg Loss: 0.6634\n",
            "Batch [90/229] Avg Loss: 1.0092\n",
            "Batch [91/229] Avg Loss: 0.9251\n",
            "Batch [92/229] Avg Loss: 0.8458\n",
            "Batch [93/229] Avg Loss: 1.1487\n",
            "Batch [94/229] Avg Loss: 1.1476\n",
            "Batch [95/229] Avg Loss: 0.9467\n",
            "Batch [96/229] Avg Loss: 0.9698\n",
            "Batch [97/229] Avg Loss: 0.9187\n",
            "Batch [98/229] Avg Loss: 1.3781\n",
            "Batch [99/229] Avg Loss: 0.9131\n",
            "Batch [100/229] Avg Loss: 0.9232\n",
            "Batch [101/229] Avg Loss: 1.1675\n",
            "Batch [102/229] Avg Loss: 1.1482\n",
            "Batch [103/229] Avg Loss: 0.8653\n",
            "Batch [104/229] Avg Loss: 1.1055\n",
            "Batch [105/229] Avg Loss: 0.8111\n",
            "Batch [106/229] Avg Loss: 0.8297\n",
            "Batch [107/229] Avg Loss: 0.9217\n",
            "Batch [108/229] Avg Loss: 1.2478\n",
            "Batch [109/229] Avg Loss: 1.1196\n",
            "Batch [110/229] Avg Loss: 1.0173\n",
            "Batch [111/229] Avg Loss: 0.8963\n",
            "Batch [112/229] Avg Loss: 0.9430\n",
            "Batch [113/229] Avg Loss: 1.3655\n",
            "Batch [114/229] Avg Loss: 0.7168\n",
            "Batch [115/229] Avg Loss: 1.2252\n",
            "Batch [116/229] Avg Loss: 0.8344\n",
            "Batch [117/229] Avg Loss: 1.2601\n",
            "Batch [118/229] Avg Loss: 0.9040\n",
            "Batch [119/229] Avg Loss: 0.6455\n",
            "Batch [120/229] Avg Loss: 1.0349\n",
            "Batch [121/229] Avg Loss: 0.7731\n",
            "Batch [122/229] Avg Loss: 0.9399\n",
            "Batch [123/229] Avg Loss: 1.1106\n",
            "Batch [124/229] Avg Loss: 0.8455\n",
            "Batch [125/229] Avg Loss: 0.9986\n",
            "Batch [126/229] Avg Loss: 0.8327\n",
            "Batch [127/229] Avg Loss: 0.6878\n",
            "Batch [128/229] Avg Loss: 0.9966\n",
            "Batch [129/229] Avg Loss: 0.7946\n",
            "Batch [130/229] Avg Loss: 0.9364\n",
            "Batch [131/229] Avg Loss: 0.8203\n",
            "Batch [132/229] Avg Loss: 0.5161\n",
            "Batch [133/229] Avg Loss: 0.9132\n",
            "Batch [134/229] Avg Loss: 1.0807\n",
            "Batch [135/229] Avg Loss: 0.6654\n",
            "Batch [136/229] Avg Loss: 0.7861\n",
            "Batch [137/229] Avg Loss: 0.8292\n",
            "Batch [138/229] Avg Loss: 1.1030\n",
            "Batch [139/229] Avg Loss: 0.7559\n",
            "Batch [140/229] Avg Loss: 0.8395\n",
            "Batch [141/229] Avg Loss: 0.5201\n",
            "Batch [142/229] Avg Loss: 0.7667\n",
            "Batch [143/229] Avg Loss: 0.8615\n",
            "Batch [144/229] Avg Loss: 0.9319\n",
            "Batch [145/229] Avg Loss: 0.9651\n",
            "Batch [146/229] Avg Loss: 0.7033\n",
            "Batch [147/229] Avg Loss: 0.6718\n",
            "Batch [148/229] Avg Loss: 0.8412\n",
            "Batch [149/229] Avg Loss: 0.7964\n",
            "Batch [150/229] Avg Loss: 0.7054\n",
            "Batch [151/229] Avg Loss: 1.3085\n",
            "Batch [152/229] Avg Loss: 0.5700\n",
            "Batch [153/229] Avg Loss: 0.8888\n",
            "Batch [154/229] Avg Loss: 0.7674\n",
            "Batch [155/229] Avg Loss: 0.9959\n",
            "Batch [156/229] Avg Loss: 0.7345\n",
            "Batch [157/229] Avg Loss: 0.8877\n",
            "Batch [158/229] Avg Loss: 0.6035\n",
            "Batch [159/229] Avg Loss: 0.6905\n",
            "Batch [160/229] Avg Loss: 0.7108\n",
            "Batch [161/229] Avg Loss: 0.9985\n",
            "Batch [162/229] Avg Loss: 0.9126\n",
            "Batch [163/229] Avg Loss: 0.7818\n",
            "Batch [164/229] Avg Loss: 0.4648\n",
            "Batch [165/229] Avg Loss: 0.7851\n",
            "Batch [166/229] Avg Loss: 0.8439\n",
            "Batch [167/229] Avg Loss: 0.7632\n",
            "Batch [168/229] Avg Loss: 0.5397\n",
            "Batch [169/229] Avg Loss: 0.6461\n",
            "Batch [170/229] Avg Loss: 0.7430\n",
            "Batch [171/229] Avg Loss: 0.7087\n",
            "Batch [172/229] Avg Loss: 0.6681\n",
            "Batch [173/229] Avg Loss: 0.7345\n",
            "Batch [174/229] Avg Loss: 0.8456\n",
            "Batch [175/229] Avg Loss: 0.9025\n",
            "Batch [176/229] Avg Loss: 0.5026\n",
            "Batch [177/229] Avg Loss: 0.7484\n",
            "Batch [178/229] Avg Loss: 0.5805\n",
            "Batch [179/229] Avg Loss: 0.5307\n",
            "Batch [180/229] Avg Loss: 0.8776\n",
            "Batch [181/229] Avg Loss: 0.4533\n",
            "Batch [182/229] Avg Loss: 0.9562\n",
            "Batch [183/229] Avg Loss: 0.4168\n",
            "Batch [184/229] Avg Loss: 0.7798\n",
            "Batch [185/229] Avg Loss: 0.6616\n",
            "Batch [186/229] Avg Loss: 0.4410\n",
            "Batch [187/229] Avg Loss: 0.5234\n",
            "Batch [188/229] Avg Loss: 0.5631\n",
            "Batch [189/229] Avg Loss: 0.6459\n",
            "Batch [190/229] Avg Loss: 0.8105\n",
            "Batch [191/229] Avg Loss: 1.0163\n",
            "Batch [192/229] Avg Loss: 0.5845\n",
            "Batch [193/229] Avg Loss: 0.7890\n",
            "Batch [194/229] Avg Loss: 0.5377\n",
            "Batch [195/229] Avg Loss: 0.7176\n",
            "Batch [196/229] Avg Loss: 0.8487\n",
            "Batch [197/229] Avg Loss: 0.4764\n",
            "Batch [198/229] Avg Loss: 0.7142\n",
            "Batch [199/229] Avg Loss: 0.8234\n",
            "Batch [200/229] Avg Loss: 0.6912\n",
            "Batch [201/229] Avg Loss: 0.7209\n",
            "Batch [202/229] Avg Loss: 0.6035\n",
            "Batch [203/229] Avg Loss: 0.6983\n",
            "Batch [204/229] Avg Loss: 0.7502\n",
            "Batch [205/229] Avg Loss: 0.6274\n",
            "Batch [206/229] Avg Loss: 0.5875\n",
            "Batch [207/229] Avg Loss: 0.5460\n",
            "Batch [208/229] Avg Loss: 0.8715\n",
            "Batch [209/229] Avg Loss: 0.7856\n",
            "Batch [210/229] Avg Loss: 0.8332\n",
            "Batch [211/229] Avg Loss: 0.6137\n",
            "Batch [212/229] Avg Loss: 0.8765\n",
            "Batch [213/229] Avg Loss: 0.8589\n",
            "Batch [214/229] Avg Loss: 0.7695\n",
            "Batch [215/229] Avg Loss: 0.6646\n",
            "Batch [216/229] Avg Loss: 0.8882\n",
            "Batch [217/229] Avg Loss: 0.5731\n",
            "Batch [218/229] Avg Loss: 0.4470\n"
          ]
        }
      ]
    }
  ]
}